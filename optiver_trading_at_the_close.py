# -*- coding: utf-8 -*-
"""Optiver_trading_at_the_close.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n4_4ZbnG0S3IyaJmYyW9h2F-8YVc-8Xx
"""

import pandas as pd
import seaborn as sns

!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json



!kaggle competitions download -c optiver-trading-at-the-close

!unzip optiver-trading-at-the-close.zip

train_df = pd.read_csv('train.csv')
print(train_df.head())

train_df.info()

train_df.describe()

import lightgbm as lgb
import numpy as np
from typing import List, Generator
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit, KFold
from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples
from sklearn.utils.validation import indexable, _num_samples, _deprecate_positional_args

!pip install dask

train_df.isna().sum()

train_df['date_id'].unique()

def create_time_features(df):
    df['minute_in_bucket'] = df['seconds_in_bucket'] // 60
    df['day_of_week'] = df['date_id'] // 5
    df['first_5min'] = 1 if df['seconds_in_bucket'] // 60 < 5 else 0
    df['last_5min'] = 1 if df['seconds_in_bucket'] // 60 >= 5 else 0
    df['first_5min'] = 1 if df['seconds_in_bucket'] <= 300 else 0
    df['last_5min'] = 1 if (df['seconds_in_bucket'] > 300 & df['seconds_in_bucket'] <= 540) else 0

    return df

def create_features(df):

    # Time based features
    df['minute_in_bucket'] = df['seconds_in_bucket'] // 60

    # Price Features
    ## Spreads in Bps
    df['mid'] = df['bid_price'].add(df['ask_price']).div(2)

    df['tob_spread_bps'] = df['bid_price'].sub(df['ask_price']).div(df['mid']) * 10000
    df['auction_spread_bps'] = df['near_price'].sub(df['far_price']).div(df['near_price'].add(df['far_price']).div(2)) * 10000

    ## Calculate difference of mid and the other price
    price_cols = ['reference_price','wap','far_price','near_price','bid_price','ask_price', 'mid']

    ## Generate combinations of price column pairs without repeats
    price_col_combinations = list(itertools.combinations(price_cols, 2))

    for col1, col2 in price_col_combinations:
        new_col_name = f'{col1}_{col2}_diff'
        df[new_col_name] = df[col1].sub(df[col2])


    # Size Features
    ## TOB and Matched/Unmatched Size Imbalance (i.e., Orderbook/Auction Pressure)
    df['tob_liquidity'] = df['bid_size'].add(df['ask_size'])
    df['signed_imbalance_size'] = df['imbalance_size'].mul(df['imbalance_buy_sell_flag'])

    df['tob_imb'] = df['bid_size'].sub(df['ask_size']).div(df['bid_size'].add(df['ask_size']))
    df['matched_imb'] = df['matched_size'].sub(df['imbalance_size']).div(df['matched_size'].add(df['imbalance_size']))
    df['auction_imb'] = df['imbalance_size'].mul(df['imbalance_buy_sell_flag']).div(df['matched_size'], fill_value=0)

    df['matched_bid_ratio'] = df['matched_size'].div(df['bid_size'])
    df['matched_ask_ratio'] = df['matched_size'].div(df['ask_size'])
    df['matched_tob_liquidity_ratio'] = df['matched_size'].div(df['tob_liquidity'])
    df['imbalance_tob_liquidity_ratio'] = df['imbalance_size'].div(df['tob_liquidity'])
    df['imbalance_ratio'] = df['imbalance_size'].div(df['matched_size'])

    size_cols = ['bid_size','ask_size','matched_size','imbalance_size']

    ## Generate combinations of price column pairs without repeats
    size_col_combinations = list(itertools.combinations(size_cols, 2))

    for col1, col2 in size_col_combinations:
        new_col_name = f'{col1}_{col2}_diff'
        df[new_col_name] = df[col1].sub(df[col2])

    # Change datatype for category to be categorical instead of numeric
    df['stock_id'] = df['stock_id'].astype('category')

    return df

def create_synthetic_index(df, col, weights):
    """
    Create synthetic index based on the given weights and stock_ids.

    Parameters:
    - df: DataFrame with columns 'date_id', 'stock_id', and col
    - col: column to create index from
    - weights: List of weights for each stock

    Returns:
    - DataFrame with an added 'synthetic_index' column
    """
    # Column name for synthetic index
    synthetic_col = f'synthetic_index_{col}'

    # Pivot the DataFrame
    pivoted_df = df.pivot(index=['date_id', 'seconds_in_bucket'], columns='stock_id', values=col)
    pivoted_df.fillna(0, inplace=True)

    # Convert the weights list into a Series
    weights_series = pd.Series(weights, index=range(len(weights)))

    # Align weights with the pivoted DataFrame columns
    aligned_weights = weights_series.reindex(pivoted_df.columns, fill_value=0)

    # Calculate the dot product
    pivoted_df[synthetic_col] = pivoted_df.dot(aligned_weights)

    # If synthetic index column already exists, drop it
    if synthetic_col in df.columns:
        df.drop(columns=[synthetic_col], inplace=True)

    # Merge with original df using index and specified column
    df = df.merge(pivoted_df[[synthetic_col]], left_on=['date_id', 'seconds_in_bucket'], right_index=True, how='left')

    return df

def create_lag_features(df):

    # Calculate Orderflow imbalance
    df['ofi'] = ((df.groupby(['stock_id', 'date_id'])['bid_price'].shift(0) >= df.groupby(['stock_id', 'date_id'])['bid_price'].shift(1)) * df['bid_size'] -
             (df.groupby(['stock_id', 'date_id'])['bid_price'].shift(0) <= df.groupby(['stock_id', 'date_id'])['bid_price'].shift(1)) * df.groupby(['stock_id', 'date_id'])['bid_size'].shift(1) -
             (df.groupby(['stock_id', 'date_id'])['ask_price'].shift(0) <= df.groupby(['stock_id', 'date_id'])['ask_price'].shift(1)) * df['ask_size'] +
             (df.groupby(['stock_id', 'date_id'])['ask_price'].shift(0) >= df.groupby(['stock_id', 'date_id'])['ask_price'].shift(1)) * df.groupby(['stock_id', 'date_id'])['ask_size'].shift(1))



    lag_periods = [1, 2, 3, 6]  # These represent the periods you're interested in
    price_cols = ['wap', 'mid', 'bid_price', 'ask_price']

    for period in lag_periods:
        for col in price_cols:
            # Calculate the lagged value for each column within each group
            df[f'{col}_lag{period}'] = df.groupby(['stock_id', 'date_id'])[col].shift(periods=period)

            # Calculate the return in basis points (bps) comparing the current mid price with the lagged price
            df[f'{col}_return_bps_lag{period*10}s'] = ((df['mid'] / df[f'{col}_lag{period}']) - 1) * 10000

            df.drop(columns=[f'{col}_lag{period}'], inplace=True)

    return df

def create_lag_index(df, col):

    lag_periods = [1, 2, 3, 6]
    col_name = f'synthetic_index_{col}'

    for period in lag_periods:
        # Calculate the lagged value for each column within each group
        df[f'{col_name}_lag{period}'] = df.groupby(['stock_id', 'date_id'])[col_name].shift(periods=period)

        # Calculate the return in basis points (bps) comparing the current mid price with the lagged price
        df[f'{col_name}_return_bps_lag{period*10}s'] = ((df[col_name] / df[f'{col_name}_lag{period}']) - 1) * 10000

        df.drop(columns=[f'{col_name}_lag{period}'], inplace=True)

    return df

def create_relative_wap_return(df, col):

    lag_periods = [1, 2, 3, 6]
    index_col_name = f'synthetic_index_{col}'
    price_col_name = col

    for period in lag_periods:
        df[f'{index_col_name}_{price_col_name}_diff'] = df[f'{index_col_name}_return_bps_lag{period*10}s'] - df[f'{price_col_name}_return_bps_lag{period*10}s']

    return df

def create_rolling_intraday_features(df):
    column_names = ['wap_mid_diff', 'reference_price_wap_diff']
    lags = [6, 12, 30]

    for col in column_names:
        for lag in lags:
            # df[f'{column_name}_rolling_{lag*10}s_mean'] = df.groupby(['stock_id', 'date_id'])[column_name].transform(lambda x: x.rolling(window=lag, min_periods=1).mean())
            df[f'{col}_rolling_{lag*10}s_median'] = df.groupby(['stock_id', 'date_id'])[col].transform(lambda x: x.rolling(window=lag, min_periods=1).median())
            # df[f'{col}_rolling_{lag*10std'] = df.groupby('stock_id')[col].transform(lambda x: x.rolling(window=lag, min_periods=1).std())

    return df

def create_rolling_intraday_sum(df):
    column_names = ['imbalance_buy_sell_flag']
    lags = [3, 6, 12, 30]

    for col in column_names:
        for lag in lags:
            df[f'{col}_rolling_{lag*10}s_sum'] = df.groupby(['stock_id', 'date_id'])[col].transform(lambda x: x.rolling(window=lag, min_periods=1).sum())

    return df

def create_momentum_features(df, column_name):

    lag_cols = [col for col in df.columns if f'{column_name}_rolling' in col or col == column_name]
    ## Generate combinations of price column pairs without repeats
    lag_cols_combinations = list(itertools.combinations(lag_cols, 2))

    for col1, col2 in lag_cols_combinations:
        new_col_name = f'{col1}_div_{col2}'
        df[new_col_name] = df[col1].div(df[col2])

    return df

def create_previous_day_median_5min(df, column_name):
    # Filter dataframe for first 5 minutes
    first_5min = df[df['seconds_in_bucket'] <= 300]

    # Filter dataframe for last 5 minutes
    last_5min = df[(df['seconds_in_bucket'] > 300) & (df['seconds_in_bucket'] <= 540)]

    # Calculate median for the given column_name
    median_first_5min = first_5min.groupby(['stock_id', 'date_id'])[column_name].median().reset_index()
    median_last_5min = last_5min.groupby(['stock_id', 'date_id'])[column_name].median().reset_index()

    # Rename columns
    median_first_5min.rename(columns={column_name: f'previous_day_first_5min_median_{column_name}'}, inplace=True)
    median_last_5min.rename(columns={column_name: f'previous_day_last_5min_median_{column_name}'}, inplace=True)

    # Shift date_id by 1 to get yesterdays data
    median_first_5min['date_id'] = median_first_5min['date_id'] + 1
    median_last_5min['date_id'] = median_last_5min['date_id'] + 1

    # Merge median values with original dataframe
    df = pd.merge(df, median_first_5min, on=['stock_id', 'date_id'], how='left')
    df = pd.merge(df, median_last_5min, on=['stock_id', 'date_id'], how='left')

    return df

def create_previous_day_median(df, column_name):
    # Calculate median for the given column_name
    stock_medians = df.groupby(['stock_id', 'date_id'])[column_name].median().reset_index(name=f'previous_day_median_{column_name}')

    # Shift date_id by 1 to get yesterdays data
    stock_medians['date_id'] = stock_medians['date_id'] + 1

    # Merge median values with original dataframe
    df = pd.merge(df, stock_medians, on=['stock_id', 'date_id'], how='left')

    return df

def create_rolling_historical_features(df, column_name, window_size):
    # Check if the 'previous_day_median_{column_name}' column exists
    if f'previous_day_median_{column_name}' not in df.columns:
        df = create_previous_day_median(df, column_name)

    # Now that we have the previous day's median, compute the rolling median
    # Reset index to ensure compatibility during merge
    df.reset_index(drop=True, inplace=True)

    # Create a DataFrame for rolling calculation with reset index
    rolling_df = df[['stock_id', 'date_id', f'previous_day_median_{column_name}']].drop_duplicates().reset_index(drop=True)
    # rolling_df.sort_values(by=['stock_id', 'date_id'], inplace=True)

    # Compute the rolling median within each stock_id
    col_name = f'historical_median_median_{column_name}'

    rolling_df[col_name] = (
        rolling_df.groupby('stock_id')[f'previous_day_median_{column_name}']
        .transform(lambda x: x.rolling(window=window_size, min_periods=3).median())
    )

    # Merge the rolling median back onto the original dataframe
    df = pd.merge(df, rolling_df[['stock_id', 'date_id', col_name]], on=['stock_id', 'date_id'], how='left')

    return df

def create_historical_comparison(df, column_name):

    df[f'{column_name}_div_historical_median_median'] = df[column_name]/ df[f'historical_median_median_{column_name}']

    return df

def feature_engineering(df, verbose=0):

    df = create_features(df)
    df = create_synthetic_index(df, 'wap', index_weights)
    df = create_lag_index(df, 'wap')
    df = create_lag_features(df)
    df = create_rolling_intraday_sum(df)
    df = reduce_mem_usage(df, verbose)
    # df = df.drop(['date_id','time_id', 'row_id'], axis=1)

    return df

def feature_engineering_1(df, verbose=0):
    df = create_features(df)
    df = create_synthetic_index(df, 'wap', index_weights)
    df = reduce_mem_usage(df, verbose)
    return df

def feature_engineering_2(df, verbose=0):
    df = create_lag_index(df, 'wap')
    df = create_lag_features(df)
    df = create_rolling_intraday_sum(df)
    df = reduce_mem_usage(df, verbose)

    return df

import itertools

def reduce_mem_usage(df, verbose=0):
    """
    Iterate through all numeric columns of a dataframe and modify the data type
    to reduce memory usage.
    """

    # Calculate the initial memory usage of the DataFrame
    start_mem = df.memory_usage().sum() / 1024**2

    # Iterate through each column in the DataFrame
    for col in df.columns:
        col_type = df[col].dtype

        # Check if the column's data type is not 'object' (i.e., numeric)
        if pd.api.types.is_numeric_dtype(col_type):
            c_min = df[col].min()
            c_max = df[col].max()

            # Check if the column's data type is an integer
            if str(col_type)[:3] == "int":
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                # Check if the column's data type is a float
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float32)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float32)

    # Provide memory optimization information if 'verbose' is True
    if verbose:
        print(f"Memory usage of dataframe is {start_mem:.2f} MB")
        end_mem = df.memory_usage().sum() / 1024**2
        print(f"Memory usage after optimization is: {end_mem:.2f} MB")
        decrease = 100 * (start_mem - end_mem) / start_mem
        print(f"Decreased by {decrease:.2f}%")

    # Return the DataFrame with optimized memory usage
    return df

index_weights = [
    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,
    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,
    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,
    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,
    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,
    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,
    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,
    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,
    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,
    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,
    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,
    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,
    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,
    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,
    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,
    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,
    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,
]

train_df = feature_engineering_1(train_df)

train_df.head()

train_df.info()

